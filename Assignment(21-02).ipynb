{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f3c55e-ec85-4594-94ee-6be6bfd5b895",
   "metadata": {},
   "source": [
    "### <center> ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4c3c7-a6b0-4948-9df3-ee58d1a39a8d",
   "metadata": {},
   "source": [
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd3bd8-e916-4ffc-9822-e1b0b1632d8a",
   "metadata": {},
   "source": [
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. Three areas where Web Scraping is used to get data are-  \n",
    "\n",
    "i. Comparison Shopping Sites: Some several websites and applications can help you to easily compare pricing between several retailers for the same product. One way that these websites work is by using web scrapers to scrape product data and pricing from each retailer daily. This way, they can provide their users with the comparison data they need.  \n",
    "\n",
    "ii. Real Estate Listing Scraping: Many real estate agents use web scraping to populate their database of available properties for sale or for rent.  \n",
    "\n",
    "iii. Training and Testing Data for Machine Learning Projects: Web Scraping helps to gather data for testing / training your Machine Learning models. Quality of the machine learning models depend on the quality of training data used and when the data is not readily available web scraping can be employed to collect it from various websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0fa46d-e491-4a68-b8d1-1de5f7b37543",
   "metadata": {},
   "source": [
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8e3b2-3b60-4786-8a1d-87be909466ba",
   "metadata": {},
   "source": [
    "Different methods used for web scraping are-  \n",
    "\n",
    "i. HTML Parsing: HTML parsing is done with JavaScript and targets linear or nested HTML pages. It is a fast and robust method that is used for text extraction, screen scraping, and resource extraction among others.  \n",
    "\n",
    "ii. DOM Parsing: DOM is short for Document Object Model and it defines the style structure and content of XML files. Scrapers make use of DOM parsers to get an in-depth view of a web pageâ€™s structure. They can also use a DOM parser to get nodes containing information and then use a tool like XPath to scrape web pages.  \n",
    "\n",
    "iii. Vertical Aggregation: Vertical aggregation platforms are created by companies with access to large scale computing power to target specific verticals. Some companies run the platforms on the cloud. Bots creation and monitoring for specific verticals are done by these platforms without any human intervention.  \n",
    "\n",
    "iv. XPath: XML Path Language is a query language that is used with XML documents. XPath can be used to navigate XML documents because of their tree-like structure by selecting nodes based on different parameters. XPath can be used together with DOM parsing to scrape an entire web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee163d2-9a3b-4c2c-81e0-a3bb4cef4abf",
   "metadata": {},
   "source": [
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aad4d5-8b3f-482b-acb1-1f9f777aced3",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner.  \n",
    "For installing Beautiful Soup we can use the PIP command below:  \n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced4311-d069-4c85-9a3c-92da8158ddc5",
   "metadata": {},
   "source": [
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a20a9-834c-41bc-a265-a20ff3c9c03d",
   "metadata": {},
   "source": [
    "Flask has been used in this project to build a simple web application to show the results of the scrapped data and store the results in mongodb backend at the same time. Firstly, it provides a working website by converting HTML and CSS files into an UI application where one can input the required query and send it back to another route as a POST request. Secondly, the entire logic for web scrapping and sending data to the backend as well as back to the website as a separate page is also written inside it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56917fdf-5f7b-49a6-9777-9431a1e8d8e2",
   "metadata": {},
   "source": [
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce914a9-36e9-4e57-90b9-bd485a5bc6d3",
   "metadata": {},
   "source": [
    "The AWS services used in this project are-  \n",
    "\n",
    "i. AWS CodePipeline: It is a fully managed continuous delivery service that helps to automate release pipelines for fast and reliable application and infrastructure updates. It has been used to connect github and AWS in this project to use the code  pushed on github for further work.  \n",
    "\n",
    "ii. AWS Elastic Beanstalk: It is an AWS managed service for web applications. Elastic beanstalk is a pre-configured EC2 server that can directly take up your application code and environment configurations and use it to automatically provision and deploy the required resources within AWS to run the web application.\n",
    "\n",
    "These two have been used in coordination in this project to obtain the code from github using CodePipeline and building an application to be used universally with Elastic Beanstalk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
